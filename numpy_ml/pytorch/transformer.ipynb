{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    #nnembedding创建一个词表。 输入的xsize为： batch_size， max_seq_length,  vocab_size\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)     # size: vocabsize, hidden_dim\n",
    "        self.sqrt_factor = math.sqrt(hidden_dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor :\n",
    "        embedding = self.embedding(x.long())\n",
    "        x = embedding * self.sqrt_factor\n",
    "        return x   # size: batch_size, max_seq_length, hidden_dim\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionalEncoding(nn.Module):\n",
    "    def __init__(self, max_positions, hidden_dim, drop_prob):\n",
    "        pe = torch.zeros(max_positions, hidden_dim)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        positions = torch.arange(max_positions).unsqueeze(1)  # [maxpositions, 1]\n",
    "        div_pair = torch.arange(0, hidden_dim, 2)\n",
    "        div_term = torch.exp(div_pair * (-math.log(10000.0)) / hidden_dim)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(positions * div_term)  # max_porsitions x 1   *   1 x hidden_dim\n",
    "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
    "\n",
    "        pe = self.pe.unsqueeze(0)  # 1,max_positions, hidden_dim\n",
    "        self.register_buffer('pe',pe)  # register as non-learnable paras\n",
    "        #定义一组参数，该组参数的特别之处在于：模型训练时不会更新（即调用 optimizer.step() 后该组参数不会变化，只可人为地改变它们的值），但是保存模型时，该组参数又作为模型参数不可或缺的一部分被保存。\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_seq_length = x.size(1)\n",
    "        x = x + self.pe[:, :max_seq_length]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask):\n",
    "    sqrt_dim = query.shape[-1] ** 0.5\n",
    "\n",
    "    scores = torch.matmul(query, key.transpose(-2,-1)) / sqrt_dim # 矩阵乘法\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0,-1e9)  # scores =0 we set to -1e9\n",
    "    weight = F.softmax(scores, dim=-1)  # 对最后一个维度（也就是 maxpositions的行做softmax， 每一行的和都为1）， 这个就是对应的attention map（在self attention中）\n",
    "    return torch.matmul(weight, value)  \n",
    "\n",
    "# about mask\n",
    "# We deal with a batch of sequences during training and need to add padding to shorter sequences. However, we do not want them for attention calculation.An attention mask has zeros for padding positions after the end of a sequence.\n",
    "# We give a large negative value to masked positions so that the softmax will assign zero probability to them. So, it would look like the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, drop_prob):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dim_head = hidden_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, y, mask):\n",
    "        q = self.query(x)\n",
    "        k = self.query(y)\n",
    "        v = self.query(y)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.dim_head)\n",
    "        key   = key.view(batch_size, -1, self.num_heads, self.dim_head)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.dim_head)\n",
    "\n",
    "        # Into the number of heads (batch_size, num_heads, -1, dim_head)\n",
    "        query = query.transpose(1, 2)\n",
    "        key   = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        attn = attention(query, key, value, mask)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim) # concat the output on num_heads together to become hidden_dim back.\n",
    "\n",
    "        out = self.dropout(self.output(attn))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, ffd_dim, drop_prob):\n",
    "        super().__init__()\n",
    "        self.ffd = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, ffd_dim),\n",
    "            nn.ReLU(inplace=True),  # to replace the variable to save the storage memory\n",
    "            nn.Dropout(drop_prob),\n",
    "            nn.Linear(ffd_dim, hidden_dim),\n",
    "            nn.Dropout(drop_prob),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ffd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, drop_prob, ffd_dim):\n",
    "        super().__init__()\n",
    "        self.LayerNorm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.attention = MultiHeadAttention(hidden_dim, num_heads, drop_prob)\n",
    "        self.LayerNorm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.feedforward = FeedForward(hidden_dim, ffd_dim, drop_prob)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x,  x_mask):\n",
    "        x = x + self.layer1(x,  x_mask)\n",
    "        x = x + self.layer2(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "    def layer1(self, x, x_mask):\n",
    "        x = self.LayerNorm1(x)\n",
    "        x = self.attention(x, x, x_mask)\n",
    "        return x\n",
    "    \n",
    "    def layer2(self, x, x_mask):\n",
    "        x = self.LayerNorm2(x)\n",
    "        x = self.feedforward(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, drop_prob, ffd_dim):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [EncoderBlock( hidden_dim, num_heads, drop_prob, ffd_dim)]\n",
    "        )\n",
    "        self.laynorm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, x_mask)\n",
    "        x = self.layernorm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, drop_prob, ffd_dim):\n",
    "        super().__init__()\n",
    "        self.LayerNorm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attn = MultiHeadAttention(hidden_dim, num_heads, drop_prob)\n",
    "        self.LayerNorm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_dim, num_heads, drop_prob)\n",
    "        self.ffd = FeedForward(hidden_dim, ffd_dim, drop_prob)\n",
    "        self.LayerNorm3 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, y, y_mask, x, x_mask):\n",
    "        y = y + self.layer1(y, y_mask)\n",
    "        y = y + self.layer2(y, x, x_mask)\n",
    "        y = y + self.layer3(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def layer1(self, y, y_mask):\n",
    "        y = self.LayerNorm1(y)\n",
    "        y = self.self_attn(y, y, y_mask)\n",
    "        return y\n",
    "\n",
    "    def layer2(self, y, x, x_mask):\n",
    "        y = self.LayerNorm2(y)\n",
    "        y = self.cross_attn(y, x, x_mask)\n",
    "        return y\n",
    "    \n",
    "    def layer3(self, y):\n",
    "        y = self.LayerNorm3(y)\n",
    "        y = self.ffd(y)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, drop_prob, ffd_dim, num_blocks) :\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [DecoderBlock(hidden_dim, num_heads, drop_prob, ffd_dim)\n",
    "            for _ in num_blocks]\n",
    "        )\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, y, x_mask, y_mask):\n",
    "        for block in self.blocks:\n",
    "            y = block(y, y_mask, x, x_mask)\n",
    "            y = self.LayerNorm(y)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, drop_prob, ffd_dim, input_vocab_size, max_positions, output_vocab_size):\n",
    "        super().__init__()\n",
    "        self.input_embedding_layer = Embedding(input_vocab_size, hidden_dim)\n",
    "        self.input_position_encoding = positionalEncoding(max_positions, hidden_dim, drop_prob)\n",
    "        self.output_embedding_layer = Embedding(output_vocab_size, hidden_dim)\n",
    "        self.output_pos_encoding = positionalEncoding(\n",
    "                                       max_positions, hidden_dim, drop_prob)\n",
    "        self.encoder = Encoder(hidden_dim, num_heads, drop_prob, ffd_dim)\n",
    "        self.decoder = Decoder(hidden_dim, num_heads, drop_prob, ffd_dim)\n",
    "        self.projection = nn.Linear(hidden_dim, output_vocab_size)\n",
    "\n",
    "        # initialize paras\n",
    "        for para in self.parameters():\n",
    "            if para.dim()>1:\n",
    "                nn.init.xavier_uniform_(para)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, y, x_mask, y_mask):\n",
    "        x = self.encode(x, x_mask)\n",
    "        y = self.decode(x, y, x_mask, y_mask)\n",
    "        return y\n",
    "\n",
    "    def encode(self, x, x_mask):\n",
    "        x = self.input_embedding_layer(x)\n",
    "        x = self.input_position_encoding(x)\n",
    "        x = self.encoder(x, x_mask)\n",
    "        return x\n",
    "    \n",
    "    def deocde(self, x, x_mask, y, y_mask):\n",
    "        y = self.output_embedding_layer(y)\n",
    "        y = self.output_pos_encoding(y)\n",
    "        y = self.decoder(x, y, x_mask, y_mask)\n",
    "        return self.projection(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.LayerNorm supplement\n",
    "class LayerNorm_ty(nn.Module):\n",
    "    def __init__(self, eps, dimension):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dimension))\n",
    "        self.beta = nn.Parameter(torch.zeros(dimension))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # why choose dim=-1?\n",
    "        var = x.var(-1, unbiased=True, keepdim=True)\n",
    "        x_out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        y_out = self.gamma * x_out + self.beta\n",
    "        return y_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
