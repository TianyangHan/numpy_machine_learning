{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, C=3, seed=None):\n",
    "        self.parameters = {}\n",
    "        self.hyperparameters = {\"C\":C, \"seed\": seed,}\n",
    "        self.is_fit = False\n",
    "        self.elbo = None\n",
    "\n",
    "    def _initialize_params(self,X):\n",
    "        N,d = X.shape\n",
    "        C = self.hyperparameters;[\"C\"]      # C is the cluster numbers, default is 3\n",
    "\n",
    "        rr = np.random.rand(C)\n",
    "\n",
    "        self.parameters = {\n",
    "            \"pi\": rr/rr.sum(),    # cluster prior, the initial posibility of example belongs to each cluster, init with the same \n",
    "            \"Q\": np.zeros((N,C)),  # variational distribution, means percentage of example belongs to each cluster\n",
    "            \"mu\": np.random.uniform(-5,10, c*d).reshape(C,d),   # cluster means, C examples and d dimensions\n",
    "            \"sigma\": np.array([np.eye(d) for _ in range(C)]),   # covariance matrix, c x d x d\n",
    "        }\n",
    "\n",
    "        self.elbo = None\n",
    "        self.is_fit = False\n",
    "    \n",
    "\n",
    "    def likelihood_loewer_bound(self, X):\n",
    "        N = X.shape[0]      # example number\n",
    "        P = self.parameters\n",
    "        C = self.hyperparameters[\"C\"]\n",
    "        pi, Q, mu, sigma = P[\"pi\"], P[\"Q\"], P[\"mu\"], P[\"sigma\"]\n",
    "\n",
    "        eps = np.finfo(float).eps   # eps: non-negative minimum number, often used to generate bias(regular term)\n",
    "        expec1, expec2 = 0.0, 0.0\n",
    "\n",
    "        for i in range(N):\n",
    "            x_i = X[i]\n",
    "\n",
    "            for c in range(C):\n",
    "                pi_k = pi[c]\n",
    "                z_nk = Q[i,c]\n",
    "                mu_k = mu[c,]\n",
    "                sigma_k = sigma[c,:,:]\n",
    "\n",
    "                log_pi_k = np.log(pi_k + eps)\n",
    "                log_p_x_i = log_gaussian_pdf(x_i,mu_k,sigma_k)\n",
    "                prob = z_nk * (log_p_x_i + log_pi_k) \n",
    "\n",
    "                expec1 += prob      # likelihood lower bound\n",
    "                expec2 += z_nk * np.log(z_nk + eps)   #  regular term do the same log operation\n",
    "        loss = expec1 - expec2\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def _E_step(self,X):\n",
    "        P = self.parameters\n",
    "        C = self.hyperparameters[\"C\"]\n",
    "        pi, Q, mu, sigma = P[\"pi\"], P[\"Q\"], P[\"mu\"], P[\"sigma\"]\n",
    "\n",
    "        for i, x_i in enumerate(X):\n",
    "            denom_vals = []\n",
    "            for c in range(C):\n",
    "                pi_c = pi[c]\n",
    "                mu_c = mu[c,:]\n",
    "                sigma_c = sigma[c,:,:]\n",
    "\n",
    "                log_pi_c = np.log(pi_c)\n",
    "                log_p_x_i = log_gaussian_pdf(x_i, mu_c, sigma_c)\n",
    "\n",
    "                # log N(X_i | mu_c, Sigma_c) + log pi_c\n",
    "                denom_vals.append(log_p_x_i + log_pi_c)     # molecular\n",
    "            \n",
    "            # log \\sum_c exp{ log N(X_i | mu_c, Sigma_c) + log pi_c } \n",
    "            log_denom = logsumexp(denom_vals)\n",
    "            q_i = np.exp([num - log_denom for num in denom_vals])\n",
    "            np.testing.assert_allclose(np.sum(q_i), 1, err_msg=\"{}\".format(np.sum(q_i)))\n",
    "\n",
    "            Q[i, :] = q_i       # record each example probability that produced by each components\n",
    "\n",
    "\n",
    "    def _M_step(self,X):\n",
    "        N, d = X.shape\n",
    "        P = self.parameters\n",
    "        C = self.hyperparameters[\"C\"]\n",
    "        pi, Q, mu, sigma = P[\"pi\"], P[\"Q\"], P[\"mu\"], P[\"sigma\"]\n",
    "\n",
    "        denoms = np.sum(Q, axis = 0)  #  denoms represents Kth cluster's posterior prob, aka the num of points of Kth cluster\n",
    "\n",
    "        #update\n",
    "        pi = denoms/N  # ak or pi\n",
    "        \n",
    "        nums_mu = [np.dot(Q[:,c], X) for c in range(C)]\n",
    "        for ix, (num,den) in enumerate(zip(nums_mu, denoms)):\n",
    "            mu[ix, :] = num/den if den > 0 else np.zeros_like(num)\n",
    "\n",
    "        for c in range(C):\n",
    "            mu_c = mu[c,:]\n",
    "            n_c = denoms[c]\n",
    "\n",
    "            outer = np.zeros((d,d))\n",
    "            for i in range(N):\n",
    "                wic = Q[i,c]\n",
    "                xi = X[i,:]\n",
    "                outer += wic * np.outer(xi-mu_c, xi-mu_c)  # caculate outer multiply\n",
    "            \n",
    "            outer = outer/n_c if n_c>0 else outer\n",
    "            sigma[c,:,:] = outer\n",
    "\n",
    "        np.testing.assert_allclose(np.sum(pi), 1, err_msg=\"{}\".format(np.sum(pi)))  # test whether sum of the prob pi is one\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, max_iter=100, tol=1e-3, verbose=False):\n",
    "        prev_vlb = -np.inf\n",
    "        self._initialize_params(X)\n",
    "\n",
    "        for _iter in range(max_iter):\n",
    "            try:\n",
    "                self._E_step(X)\n",
    "                self._M_step(X)\n",
    "                vlb = self.likelihood_loewer_bound(X)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"{_iter + 1}. Lower bound: {vlb}\")\n",
    "                \n",
    "                converged = _iter>0 and np.abs(vlb - prev_vlb)<=tol\n",
    "                if np.isnan(vlb) or converged:\n",
    "                    break\n",
    "\n",
    "                prev_vlb = vlb\n",
    "\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"Singular matrix: components collapsed\")\n",
    "                return -1\n",
    "\n",
    "        self.elbo = vlb\n",
    "        self.is_fit = True\n",
    "        return 0\n",
    "\n",
    "\n",
    "    \n",
    "    def predict(self,X, soft_labels=True):\n",
    "        assert self.is_fit\n",
    "\n",
    "        P = self.parameters\n",
    "        C = self.hyperparameters[\"C\"]\n",
    "        mu, sigma = P[\"mu\"], P[\"sigma\"]\n",
    "\n",
    "        y = []\n",
    "        for x_i in X:\n",
    "            cprobs = [log_gaussian_pdf(x_i, mu[c,:], sigma[c,:,:]) for c in range(C)]\n",
    "\n",
    "            if not soft_labels:\n",
    "                y.append(np.argmax(cprobs))\n",
    "            else:\n",
    "                y.argmax(cprobs)\n",
    "\n",
    "        return np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$loss = \\sum_{i=1}^{K}\\log_{}{(\\sum_{k=1}^{K}\\pi_{k}N(x_{i}|\\mu_{k},\\sigma_{k}))}$    \n",
    "\n",
    "lack of log regular term\n",
    "\n",
    "\n",
    "molecular : $log N(X_i | mu_c, Sigma_c) + \\log_{}{pi_c}$\n",
    "\n",
    "denominator : $log \\sum_c exp{( \\log_{}{N(X_i | mu_c, Sigma_c) + \\log_{}{pi_c}}) } $\n",
    "\n",
    "$Q = \\frac_{molucular}{denominator}#\n",
    "\n",
    "\n",
    "\n",
    "$N_k = \\sum_{i=1}^{N}Q(i,k)$\n",
    "\n",
    "$pi_k = N_k / N$\n",
    "\n",
    "$\\sum_{k}{} = \\frac{1}{N_k}\\sum_{i=1}^{N}Q(i,k)(x_i-\\mu_k)(x_i-\\mu_k)^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gaussian_pdf(x_i,mu,sigma):\n",
    "    \"compute log N(x_i | mu,sigma)\"\n",
    "    n = len(mu)     # C\n",
    "    a = n*np.log(2*np.pi)\n",
    "    _, b = np.linalg.slogdet(sigma)   # det(sigma)^(2)\n",
    "\n",
    "    y = np.linalg.solve(sigma, x_i - mu)   # solve a matrix equation: sigma * x = xi - mu, then x = (xi-mu) * sigma^(-1) \n",
    "    c = np.dot(x_i - mu, y)    #exp balabala\n",
    "    return -0.5 * (a+b+c)\n",
    "    \n",
    "\n",
    "def logsumexp(log_probs, axis=None):\n",
    "    _max = np.max(log_probs)\n",
    "    ds = log_probs - _max\n",
    "    exp_sum = np.exp(ds).sum(axis = axis)\n",
    "    return _max+np.log(exp_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_{j}(x;\\mu_{j}, \\sigma_{j}) = \\frac{1}{\\sqrt{(2\\pi)}^{m}\\left | \\sigma_{j} \\right |  } exp[-\\frac{1}{2}(x-\\mu_{j})^{T}\\sigma_{j}^{-1}(x-\\mu_{j})]  $\n",
    "\n",
    "doing log on N(x|mu,sigma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}